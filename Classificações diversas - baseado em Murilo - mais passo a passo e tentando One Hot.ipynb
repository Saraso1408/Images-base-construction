{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sara/lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import libopf_py\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def kfoldcv(indices, k = 10, seed = 42):\n",
    "#    subset = 0\n",
    "    \n",
    "#    size = len(indices)\n",
    "#    subset_size = round(size / k)\n",
    "#    random.Random(seed).shuffle(indices) \n",
    "    \n",
    "#    subsets = [indices[x:x+subset_size] for x in range(0, len(indices), subset_size)]    \n",
    "    \n",
    "#    kfolds = []\n",
    "#    for i in range(k):\n",
    "#        test = subsets[i]\n",
    " #       train = []\n",
    "#        for subset in subsets:\n",
    "#            if subset != test.all():\n",
    "#                train.append(subset)\n",
    "#        kfolds.append((train,test))\n",
    "       \n",
    "#    return kfolds\n",
    "\n",
    "#k = kfoldcv(labels_num)\n",
    "#print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_emotions(labels_num):\n",
    "    i = 0\n",
    "    labels = []\n",
    "    while (i < 1924):\n",
    "        if labels_num[i] == 0.0:\n",
    "            labels.append(\"neutro\") \n",
    "        if labels_num[i] == 2.0:\n",
    "            labels.append(\"alegria\")\n",
    "        if labels_num[i] == 9.0:\n",
    "            labels.append(\"surpresa\")\n",
    "        if labels_num[i] == 12.0:\n",
    "            labels.append(\"tristeza\")\n",
    "        if labels_num[i] == 13.0:\n",
    "            labels.append(\"medo\")\n",
    "        if labels_num[i] == 21.0:\n",
    "            labels.append(\"nojo\")\n",
    "        if labels_num[i] == 22.0:\n",
    "            labels.append(\"raiva\")\n",
    "        i += 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1      2      3      4      5      6      7      8      9      10   \\\n",
      "1      93.0  212.0   96.0  262.0  104.0  310.0  110.0  358.0  121.0  406.0   \n",
      "2      92.0  215.0   96.0  265.0  105.0  314.0  111.0  362.0  122.0  409.0   \n",
      "3      95.0  218.0   99.0  268.0  106.0  318.0  112.0  366.0  123.0  412.0   \n",
      "4      92.0  224.0   94.0  276.0  102.0  327.0  109.0  376.0  121.0  423.0   \n",
      "5      85.0  214.0   87.0  267.0   95.0  319.0  103.0  370.0  116.0  419.0   \n",
      "6      87.0  219.0   91.0  272.0   99.0  323.0  108.0  374.0  120.0  424.0   \n",
      "7      85.0  216.0   89.0  267.0   97.0  317.0  105.0  367.0  116.0  416.0   \n",
      "8      87.0  214.0   90.0  266.0   98.0  316.0  107.0  365.0  119.0  412.0   \n",
      "9      93.0  218.0   96.0  269.0  104.0  319.0  109.0  367.0  120.0  415.0   \n",
      "10     88.0  215.0   92.0  265.0   99.0  314.0  105.0  362.0  115.0  410.0   \n",
      "11     88.0  208.0   92.0  259.0   99.0  309.0  104.0  358.0  115.0  406.0   \n",
      "12     95.0  218.0   98.0  271.0  105.0  323.0  111.0  373.0  122.0  421.0   \n",
      "13     96.0  218.0   99.0  270.0  108.0  321.0  114.0  370.0  125.0  418.0   \n",
      "14     91.0  208.0   94.0  258.0  101.0  308.0  109.0  357.0  121.0  405.0   \n",
      "15     87.0  205.0   89.0  256.0   97.0  306.0  104.0  354.0  115.0  402.0   \n",
      "16     85.0  209.0   88.0  258.0   96.0  307.0  101.0  355.0  111.0  403.0   \n",
      "17     85.0  207.0   88.0  256.0   95.0  303.0   99.0  351.0  109.0  399.0   \n",
      "18     90.0  208.0   93.0  257.0   99.0  305.0  104.0  353.0  115.0  401.0   \n",
      "19     83.0  208.0   86.0  259.0   93.0  309.0  100.0  358.0  111.0  405.0   \n",
      "20     92.0  210.0   94.0  260.0  101.0  309.0  107.0  358.0  118.0  407.0   \n",
      "21     95.0  208.0   97.0  259.0  103.0  309.0  110.0  359.0  122.0  407.0   \n",
      "22     97.0  207.0   99.0  258.0  106.0  307.0  112.0  356.0  122.0  405.0   \n",
      "23     93.0  209.0   97.0  261.0  104.0  310.0  110.0  360.0  122.0  408.0   \n",
      "24     87.0  209.0   90.0  260.0   98.0  308.0  103.0  356.0  113.0  402.0   \n",
      "25     94.0  213.0   96.0  263.0  103.0  313.0  109.0  362.0  119.0  410.0   \n",
      "26     89.0  212.0   91.0  264.0   99.0  315.0  106.0  364.0  117.0  413.0   \n",
      "27     94.0  217.0   98.0  268.0  106.0  318.0  112.0  367.0  124.0  415.0   \n",
      "28    101.0  213.0  104.0  264.0  112.0  313.0  118.0  361.0  129.0  409.0   \n",
      "29     97.0  203.0   99.0  255.0  106.0  305.0  112.0  354.0  122.0  403.0   \n",
      "30     97.0  210.0   99.0  261.0  104.0  311.0  108.0  360.0  116.0  408.0   \n",
      "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "1895   84.0  220.0   89.0  273.0  100.0  323.0  109.0  373.0  125.0  421.0   \n",
      "1896   77.0  215.0   82.0  266.0   91.0  315.0  100.0  364.0  114.0  414.0   \n",
      "1897   82.0  209.0   86.0  262.0   95.0  312.0  103.0  362.0  116.0  411.0   \n",
      "1898   81.0  206.0   84.0  258.0   93.0  308.0  100.0  357.0  112.0  406.0   \n",
      "1899   85.0  219.0   89.0  271.0   98.0  320.0  107.0  368.0  120.0  417.0   \n",
      "1900   80.0  227.0   86.0  279.0   98.0  328.0  109.0  377.0  125.0  424.0   \n",
      "1901   76.0  221.0   81.0  273.0   93.0  322.0  104.0  371.0  118.0  419.0   \n",
      "1902   72.0  219.0   77.0  271.0   87.0  320.0   97.0  369.0  111.0  416.0   \n",
      "1903   83.0  198.0   85.0  250.0   92.0  301.0   97.0  351.0  107.0  401.0   \n",
      "1904   82.0  206.0   83.0  258.0   89.0  309.0   94.0  359.0  106.0  408.0   \n",
      "1905   85.0  219.0   88.0  271.0   93.0  322.0  100.0  373.0  115.0  422.0   \n",
      "1906   72.0  215.0   76.0  269.0   85.0  319.0   95.0  369.0  111.0  417.0   \n",
      "1907   72.0  217.0   77.0  271.0   87.0  322.0   97.0  372.0  112.0  420.0   \n",
      "1908   74.0  216.0   78.0  270.0   86.0  322.0   96.0  372.0  109.0  424.0   \n",
      "1909   65.0  230.0   72.0  284.0   83.0  334.0   95.0  383.0  111.0  432.0   \n",
      "1910   70.0  241.0   78.0  291.0   89.0  338.0   99.0  385.0  112.0  432.0   \n",
      "1911   77.0  233.0   84.0  285.0   96.0  333.0  105.0  379.0  118.0  426.0   \n",
      "1912   76.0  225.0   80.0  276.0   89.0  324.0   97.0  372.0  110.0  420.0   \n",
      "1913   73.0  220.0   78.0  271.0   87.0  319.0   96.0  367.0  110.0  415.0   \n",
      "1914   59.0  212.0   64.0  263.0   72.0  311.0   81.0  360.0   94.0  408.0   \n",
      "1915   63.0  203.0   66.0  253.0   74.0  303.0   80.0  351.0   92.0  400.0   \n",
      "1916   75.0  217.0   80.0  268.0   89.0  317.0   97.0  364.0  111.0  411.0   \n",
      "1917   70.0  214.0   73.0  264.0   80.0  311.0   86.0  359.0   99.0  405.0   \n",
      "1918   79.0  213.0   83.0  265.0   91.0  314.0   98.0  363.0  109.0  411.0   \n",
      "1919   86.0  218.0   90.0  266.0   98.0  313.0  105.0  359.0  116.0  406.0   \n",
      "1920   80.0  218.0   84.0  266.0   93.0  313.0  100.0  359.0  110.0  407.0   \n",
      "1921   85.0  219.0   90.0  269.0   98.0  317.0  105.0  364.0  116.0  411.0   \n",
      "1922   75.0  213.0   79.0  261.0   87.0  308.0   94.0  355.0  105.0  402.0   \n",
      "1923   83.0  211.0   87.0  261.0   94.0  308.0  101.0  356.0  114.0  403.0   \n",
      "1924   71.0  202.0   75.0  251.0   83.0  299.0   90.0  347.0  102.0  394.0   \n",
      "\n",
      "      ...    128    129    130    131    132    133    134    135    136 137  \n",
      "1     ...  408.0  321.0  410.0  285.0  407.0  268.0  408.0  251.0  405.0   0  \n",
      "2     ...  410.0  322.0  412.0  285.0  410.0  268.0  411.0  251.0  408.0   0  \n",
      "3     ...  414.0  321.0  415.0  287.0  412.0  270.0  413.0  253.0  410.0   0  \n",
      "4     ...  413.0  315.0  421.0  284.0  426.0  266.0  428.0  248.0  425.0   0  \n",
      "5     ...  408.0  305.0  419.0  278.0  431.0  262.0  433.0  246.0  431.0   0  \n",
      "6     ...  417.0  302.0  430.0  276.0  430.0  263.0  431.0  249.0  429.0   0  \n",
      "7     ...  418.0  302.0  427.0  273.0  424.0  259.0  425.0  245.0  422.0   0  \n",
      "8     ...  415.0  306.0  422.0  275.0  419.0  261.0  420.0  247.0  417.0   0  \n",
      "9     ...  396.0  319.0  408.0  284.0  418.0  265.0  420.0  247.0  418.0   0  \n",
      "10    ...  394.0  318.0  404.0  280.0  415.0  261.0  416.0  242.0  414.0   0  \n",
      "11    ...  397.0  317.0  405.0  278.0  417.0  258.0  418.0  239.0  415.0   0  \n",
      "12    ...  402.0  319.0  418.0  285.0  430.0  266.0  432.0  247.0  428.0   0  \n",
      "13    ...  408.0  317.0  420.0  287.0  421.0  270.0  422.0  253.0  420.0   0  \n",
      "14    ...  411.0  310.0  422.0  279.0  419.0  264.0  420.0  249.0  417.0   0  \n",
      "15    ...  411.0  302.0  420.0  271.0  418.0  258.0  418.0  244.0  415.0   0  \n",
      "16    ...  407.0  311.0  409.0  275.0  412.0  258.0  413.0  240.0  410.0   0  \n",
      "17    ...  406.0  315.0  402.0  274.0  403.0  258.0  404.0  241.0  401.0   0  \n",
      "18    ...  405.0  311.0  406.0  274.0  402.0  260.0  403.0  246.0  400.0   0  \n",
      "19    ...  403.0  305.0  409.0  272.0  412.0  256.0  414.0  239.0  411.0   0  \n",
      "20    ...  419.0  308.0  423.0  276.0  419.0  261.0  420.0  246.0  416.0   0  \n",
      "21    ...  420.0  304.0  427.0  276.0  424.0  263.0  425.0  251.0  422.0   0  \n",
      "22    ...  406.0  314.0  422.0  283.0  428.0  267.0  429.0  250.0  427.0   0  \n",
      "23    ...  399.0  319.0  411.0  284.0  424.0  266.0  425.0  248.0  423.0   0  \n",
      "24    ...  398.0  313.0  407.0  277.0  412.0  259.0  413.0  240.0  410.0   0  \n",
      "25    ...  403.0  314.0  416.0  281.0  418.0  264.0  419.0  246.0  415.0   0  \n",
      "26    ...  396.0  315.0  416.0  280.0  431.0  260.0  433.0  240.0  429.0   0  \n",
      "27    ...  395.0  329.0  408.0  287.0  431.0  266.0  433.0  244.0  430.0   0  \n",
      "28    ...  407.0  315.0  421.0  286.0  423.0  270.0  425.0  254.0  422.0   0  \n",
      "29    ...  397.0  314.0  416.0  282.0  428.0  264.0  429.0  246.0  427.0   0  \n",
      "30    ...  400.0  321.0  411.0  280.0  422.0  258.0  423.0  237.0  420.0   0  \n",
      "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  ..  \n",
      "1895  ...  387.0  325.0  415.0  296.0  435.0  277.0  437.0  258.0  436.0  22  \n",
      "1896  ...  384.0  317.0  410.0  283.0  434.0  265.0  436.0  245.0  434.0  22  \n",
      "1897  ...  408.0  303.0  427.0  275.0  427.0  261.0  428.0  246.0  426.0  22  \n",
      "1898  ...  408.0  302.0  424.0  277.0  427.0  262.0  428.0  246.0  426.0  22  \n",
      "1899  ...  409.0  314.0  418.0  284.0  415.0  269.0  416.0  254.0  415.0  22  \n",
      "1900  ...  415.0  313.0  426.0  287.0  434.0  272.0  437.0  257.0  436.0  22  \n",
      "1901  ...  416.0  304.0  431.0  280.0  431.0  267.0  433.0  253.0  432.0  22  \n",
      "1902  ...  414.0  303.0  426.0  277.0  424.0  263.0  426.0  249.0  425.0  22  \n",
      "1903  ...  390.0  309.0  411.0  275.0  437.0  255.0  438.0  234.0  436.0  22  \n",
      "1904  ...  392.0  316.0  405.0  276.0  426.0  256.0  428.0  234.0  424.0  22  \n",
      "1905  ...  393.0  320.0  415.0  291.0  442.0  271.0  444.0  250.0  442.0  22  \n",
      "1906  ...  414.0  312.0  421.0  291.0  418.0  277.0  421.0  262.0  418.0  22  \n",
      "1907  ...  414.0  309.0  422.0  283.0  419.0  268.0  421.0  253.0  419.0  22  \n",
      "1908  ...  414.0  318.0  421.0  287.0  437.0  268.0  439.0  249.0  438.0  22  \n",
      "1909  ...  409.0  310.0  416.0  285.0  419.0  272.0  421.0  258.0  421.0  22  \n",
      "1910  ...  383.0  329.0  399.0  289.0  412.0  268.0  414.0  246.0  413.0  22  \n",
      "1911  ...  379.0  325.0  405.0  289.0  419.0  270.0  421.0  250.0  419.0  22  \n",
      "1912  ...  410.0  316.0  409.0  279.0  411.0  262.0  413.0  245.0  410.0  22  \n",
      "1913  ...  411.0  302.0  425.0  276.0  420.0  262.0  421.0  247.0  419.0  22  \n",
      "1914  ...  381.0  303.0  404.0  271.0  433.0  250.0  436.0  227.0  434.0  22  \n",
      "1915  ...  374.0  311.0  389.0  269.0  410.0  245.0  411.0  220.0  409.0  22  \n",
      "1916  ...  377.0  314.0  402.0  283.0  422.0  263.0  424.0  242.0  422.0  22  \n",
      "1917  ...  380.0  317.0  389.0  273.0  409.0  251.0  410.0  228.0  408.0  22  \n",
      "1918  ...  388.0  311.0  406.0  279.0  420.0  259.0  423.0  238.0  421.0  22  \n",
      "1919  ...  404.0  314.0  411.0  281.0  405.0  266.0  406.0  250.0  405.0  22  \n",
      "1920  ...  399.0  308.0  410.0  276.0  409.0  260.0  410.0  244.0  409.0  22  \n",
      "1921  ...  405.0  313.0  414.0  282.0  410.0  266.0  411.0  249.0  410.0  22  \n",
      "1922  ...  402.0  306.0  405.0  271.0  403.0  255.0  404.0  238.0  402.0  22  \n",
      "1923  ...  406.0  315.0  407.0  277.0  406.0  261.0  407.0  244.0  405.0  22  \n",
      "1924  ...  396.0  305.0  396.0  268.0  395.0  251.0  396.0  233.0  394.0  22  \n",
      "\n",
      "[1924 rows x 137 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"csv_dataset/dataframe_7emocoes com numeros.csv\", header=None, sep=';')\n",
    "df = df.drop([0], axis=0)\n",
    "df = df.drop(columns=[0])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 93. 212.  96. ... 408. 251. 405.]\n",
      " [ 92. 215.  96. ... 411. 251. 408.]\n",
      " [ 95. 218.  99. ... 413. 253. 410.]\n",
      " ...\n",
      " [ 75. 213.  79. ... 404. 238. 402.]\n",
      " [ 83. 211.  87. ... 407. 244. 405.]\n",
      " [ 71. 202.  75. ... 396. 233. 394.]]\n",
      "1924\n"
     ]
    }
   ],
   "source": [
    "features = df.iloc[0:1925,0:136]\n",
    "features = features.to_numpy()\n",
    "features = features.astype('float64')\n",
    "\n",
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ... 22. 22. 22.]\n",
      "1924\n"
     ]
    }
   ],
   "source": [
    "labels = df.iloc[0:,136]\n",
    "labels = labels.to_numpy()\n",
    "labels = labels.astype('float64')\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'neutro', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'alegria', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'surpresa', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'tristeza', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'medo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'nojo', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva', 'raiva']\n"
     ]
    }
   ],
   "source": [
    "#labels = names_emotions(labels)\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1924\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(features)\n",
    "print(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptações ao OPF.\n",
    "def opf():\n",
    "        \n",
    "    # OPF only supports 32 bits labels at the moment\n",
    "    label_train_32 = train_labels.astype(np.int32)\n",
    "    label_test_32 = test_labels.astype(np.int32)\n",
    "\n",
    "    O = libopf_py.OPF()\n",
    "\n",
    "    O.fit(train_feat, label_train_32)\n",
    "\n",
    "    predicted = O.predict(test_feat)\n",
    "    name = \"OPF\"\n",
    "    acc = accuracy_score(label_test_32, predicted)\n",
    "    print (\"{:46} {:.3f} \".format(name, acc.mean()))\n",
    "    \n",
    "    #score_opf = cross_val_score(O,features,labels,cv=10)\n",
    "    #print(\"{:41} {:.3f} (+/- {:.3f}) \".format(name, score_opf.mean(), score_opf.std() * 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 1539 samples\n",
      "Test set: 385 samples\n"
     ]
    }
   ],
   "source": [
    "train_feat,test_feat,train_labels,test_labels = train_test_split(features,labels, test_size=0.20,random_state=42 )\n",
    "\n",
    "print (\"Training set:\", train_feat.shape[0], \"samples\")\n",
    "print (\"Test set:\", test_feat.shape[0], \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def one_hot(labels):\n",
    "    \n",
    "#    values = array(labels)\n",
    "\n",
    "    #print(values)\n",
    "    #print(len(values))\n",
    "    \n",
    "    # integer encode\n",
    "#    label_encoder = LabelEncoder()\n",
    "#    integer_encoded1 = label_encoder.fit_transform(values)\n",
    "    #print(\"1111111111111\", integer_encoded1)\n",
    "    #print(type(integer_encoded1))\n",
    "    \n",
    "    # binary encode\n",
    "#    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "#    integer_encoded = integer_encoded1.reshape(len(integer_encoded1), 1)\n",
    "#    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    #print(onehot_encoded)\n",
    "    #print(type(onehot_encoded))\n",
    "#    return onehot_encoded\n",
    "    #return integer_encoded1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def invert_oh():\n",
    "    # invert\n",
    "#    label_encoder = LabelEncoder()\n",
    "#    integer_encoded1 = label_encoder.fit_transform(values)\n",
    "#    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "#    integer_encoded = integer_encoded1.reshape(len(integer_encoded1), 1)\n",
    "#    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "#    inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "#    return inverted\n",
    "    #print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_labels_oh = one_hot(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_labels_oh = one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_labels)\n",
    "#print(test_labels)\n",
    "#print(train_labels_oh)\n",
    "#print(test_labels_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=1\n",
    "models = (SVC(kernel='linear', C=C, gamma='auto'),\n",
    "          SVC(kernel='rbf', gamma=0.5, C=C),\n",
    "          SVC(kernel='poly', degree=3, C=C, gamma='auto'),\n",
    "          KNeighborsClassifier(3),\n",
    "          DecisionTreeClassifier(max_depth=8),\n",
    "          RandomForestClassifier(max_depth=8, n_estimators=10, max_features=1),          \n",
    "          AdaBoostClassifier(),\n",
    "          GaussianNB(),\n",
    "         )\n",
    "\n",
    "names = [\"SVC with linear kernel\",\"SVC with RBF kernel\",\"SVC with polynomial (degree 3) kernel\",\n",
    "         \"K Nearest Neighbors\",\"Decision Tree\", \"Random Forest\", \"AdaBoost\",\"Naive Bayes\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-holdout validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Classifiers:                           Accuracy:\n",
      "SVC with linear kernel                         0.769 \n",
      "SVC with RBF kernel                            0.156 \n",
      "SVC with polynomial (degree 3) kernel          0.792 \n",
      "K Nearest Neighbors                            0.540 \n",
      "Decision Tree                                  0.460 \n",
      "Random Forest                                  0.460 \n",
      "AdaBoost                                       0.444 \n",
      "Naive Bayes                                    0.403 \n",
      "OPF                                            0.545 \n"
     ]
    }
   ],
   "source": [
    "print (\"      Classifiers: \\t Accuracy:\".expandtabs(44))\n",
    "for name, clf in zip(names, models):    \n",
    "    clf.fit(train_feat,train_labels) #train each model\n",
    "    scores = clf.score(test_feat,test_labels) #evaluate each model in the test set\n",
    "    print (\"{:46} {:.3f} \".format(name, scores.mean()))\n",
    "opf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Classifiers:                    Accuracy (standard deviation)\n",
      "SVC with linear kernel                    0.700 (+/- 0.123) \n",
      "SVC with RBF kernel                       0.190 (+/- 0.002) \n",
      "SVC with polynomial (degree 3) kernel     0.706 (+/- 0.112) \n",
      "K Nearest Neighbors                       0.412 (+/- 0.097) \n",
      "Decision Tree                             0.397 (+/- 0.103) \n",
      "Random Forest                             0.419 (+/- 0.113) \n",
      "AdaBoost                                  0.408 (+/- 0.113) \n",
      "Naive Bayes                               0.387 (+/- 0.125) \n"
     ]
    }
   ],
   "source": [
    "# vou deixar a cross validation inativa por enquanto.\n",
    "\n",
    "print (\"      Classifiers: \\t Accuracy (standard deviation)\".expandtabs(37))\n",
    "for name, clf in zip(names, models):    \n",
    "    scores = cross_val_score(clf,features,labels,cv=10) #train and evaluate each model in each fold    \n",
    "    print (\"{:41} {:.3f} (+/- {:.3f}) \".format(name, scores.mean(), scores.std() * 2))\n",
    "    \n",
    "#opf()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
